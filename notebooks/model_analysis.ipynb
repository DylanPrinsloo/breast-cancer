{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports essential libraries for handling arrays, images, JSON files, deep learning (Keras), and performance metrics (scikit-learn).\n",
    "\n",
    "Improvement: If youâ€™re only using certain functions from libraries (like pandas), consider importing only those to reduce overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(data):\n",
    "    categories = {cat['id']: cat['name'] for cat in data['categories']}\n",
    "    images = {img['id']: img['file_name'] for img in data['images']}\n",
    "    return categories, images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Images and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_mask(image_path, mask_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads an image and its corresponding mask using OpenCV.\n",
    "\n",
    "Improvement: You could add an assert statement to check if files are loaded correctly to avoid issues downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_masks(images, masks, categories, category_names):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(10, 10))\n",
    "    for i, (image_path, mask_path, category) in enumerate(zip(images, masks, categories)):\n",
    "        image, mask = load_image_and_mask(image_path, mask_path)\n",
    "        row, col = divmod(i, 4)\n",
    "        ax_image = axes[0, col]\n",
    "        ax_image.imshow(image)\n",
    "        ax_image.axis('off')\n",
    "        ax_image.set_title(f\"Category: {category_names[category]}\")\n",
    "        ax_mask = axes[1, col]\n",
    "        ax_mask.imshow(image)\n",
    "        ax_mask.imshow(mask, cmap='jet', alpha=0.55)\n",
    "        ax_mask.axis('off')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots a grid of images and their corresponding masks with category names.\n",
    "\n",
    "Improvement: You could add input validation to ensure that the lengths of images, masks, and categories are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Image and Mask Using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_mask_pil(image_path, mask_path, target_size=(256, 256)):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    mask = Image.open(mask_path).convert('L')\n",
    "    image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    mask = mask.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    image = np.array(image)\n",
    "    mask = np.array(mask)\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses PIL to load and resize images, converting them into arrays.\n",
    "\n",
    "Improvement: Add error handling for potential issues with file paths or unsupported image formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Batches of Images and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(images, masks):\n",
    "    batch_size = len(images)\n",
    "    fig, axes = plt.subplots(batch_size, 2, figsize=(10, batch_size * 5))\n",
    "    for i in range(batch_size):\n",
    "        ax_image = axes[i, 0]\n",
    "        ax_image.imshow(images[i])\n",
    "        ax_image.axis('off')\n",
    "        ax_mask = axes[i, 1]\n",
    "        ax_mask.imshow(masks[i], cmap='gray')\n",
    "        ax_mask.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays a batch of images and masks side by side.\n",
    "\n",
    "Improvement: Check if images and masks lists are non-empty to prevent potential errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(image_dir, mask_dir, annotations, batch_size, target_size=(256, 256)):\n",
    "    image_info = annotations['images']\n",
    "    while True:\n",
    "        np.random.shuffle(image_info)\n",
    "        for batch_start in range(0, len(image_info), batch_size):\n",
    "            images = []\n",
    "            masks = []\n",
    "            for i in range(batch_start, min(batch_start + batch_size, len(image_info))):\n",
    "                image_data = image_info[i]\n",
    "                image_filename = image_data['file_name']\n",
    "                image_path = os.path.join(image_dir, image_filename)\n",
    "                mask_filename = f\"{image_filename}_mask.png\"\n",
    "                mask_path = os.path.join(mask_dir, mask_filename)\n",
    "                try:\n",
    "                    image, mask = load_image_and_mask_pil(image_path, mask_path, target_size)\n",
    "                except (FileNotFoundError, ValueError) as e:\n",
    "                    print(f\"Error loading image or mask: {e}\")\n",
    "                    continue\n",
    "                images.append(image / 255.0)\n",
    "                masks.append(mask / 255.0)\n",
    "            yield np.array(images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates batches of images and masks for training.\n",
    "\n",
    "Improvement: Include optional augmentation for training images to increase dataset variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U-Net Model with VGG16 Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_vgg16_model(input_shape):\n",
    "    vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in vgg_base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = vgg_base.input\n",
    "    c1 = vgg_base.get_layer('block1_conv2').output\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    c2 = vgg_base.get_layer('block2_conv2').output\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    c3 = vgg_base.get_layer('block3_conv3').output\n",
    "    c4 = vgg_base.get_layer('block4_conv3').output\n",
    "\n",
    "    u5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u5 = concatenate([u5, c3])\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c2])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c1])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c7)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a U-Net model for image segmentation using VGG16 as the encoder backbone.\n",
    "\n",
    "Improvement: Experiment with making different layers trainable to see if performance improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(history):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy during Training and Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss during Training and Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizes the training and validation metrics over epochs.\n",
    "\n",
    "Improvement: Add smoothing to the plots to make trends clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_gen, steps):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(steps):\n",
    "        x_batch, y_batch = next(data_gen)\n",
    "        predictions = model.predict(x_batch)\n",
    "        predictions_bin = (predictions >= 0.5).astype(np.int32)\n",
    "        y_true.extend(y_batch.flatten())\n",
    "        y_pred.extend(predictions_bin.flatten())\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates F1 score and AUC for model evaluation.\n",
    "\n",
    "Improvement: Add additional metrics, like IoU (Intersection over Union), which is common for segmentation tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
